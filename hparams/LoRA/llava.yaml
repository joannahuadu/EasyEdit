alg_name: "LoRA"
# name: liuhaotian/llava-v1.5-7b
name: /public/home/wang_mq22/.cache/huggingface/hub/4481d270cc22fd5c4d1bb5df129622006ccd9234
model_name: "llava"
model_class: LlamaForCausalLM
tokenizer_class: LlamaTokenizer
# tokenizer_name: liuhaotian/llava-v1.5-7b
tokenizer_name: /public/home/wang_mq22/.cache/huggingface/hub/4481d270cc22fd5c4d1bb5df129622006ccd9234
cache_dir: /home/lishichao/.cache/huggingface/hub/
device: 0

lora_type: "corda"
layers: [7]
num_steps: 70
batch_size: 1
max_length: 30
lr: 5e-3
weight_decay: 0
kl_factor: 0
rank: 8
lora_alpha: 32
lora_dropout: 0.1
norm_constraint: false
target_modules: ["up_proj", "down_proj"] #["q_proj", "v_proj"]  #["up_proj", "down_proj"] #["q_proj", "v_proj"]
model_parallel: false

# image
coco_image: /public/home/wang_mq22/edit_data
rephrase_image: /public/home/wang_mq22/edit_data
result_dir: /public/home/wang_mq22/edit_data/results
train_annotation_path: /public/home/wang_mq22/edit_data/editing-data/vqa/vqa_train.json
eval_annotation_path: /public/home/wang_mq22/edit_data/editing-data/vqa/vqa_eval.json

# Evaluation
all_metrics_name: 'all_metrics_ada_layer7_updown_corda.jsonl'
json_dir: /public/home/wang_mq22/EasyEdit/results/jsonl
continuous_sample: 1

real_world_eval: true
api_key: "/public/home/wang_mq22/edit_data/api_key.json"