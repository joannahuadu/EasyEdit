alg_name: "LoRA"
name: liuhaotian/llava-v1.5-7b
model_name: "llava"
model_class: LlamaForCausalLM
tokenizer_class: LlamaTokenizer
tokenizer_name: liuhaotian/llava-v1.5-7b
cache_dir: /home/wmq/.cache/huggingface/hub/
device: 0

lora_type: "adalora"
layers: [7]
num_steps: 70
batch_size: 1
max_length: 30
lr: 5e-3
weight_decay: 0
kl_factor: 0
rank: 8
lora_alpha: 32
lora_dropout: 0.1
norm_constraint: false
target_modules: ["up_proj", "down_proj"] #["q_proj", "v_proj"]  #["up_proj", "down_proj"] #["q_proj", "v_proj"]
model_parallel: false


# image
coco_image: /mnt/data2/wmq
rephrase_image: /mnt/data2/wmq
result_dir: /data/lishichao/data/model_edit/results
train_annotation_path: /mnt/data2/wmq/editing-data/vqa/vqa_train.json
eval_annotation_path: /mnt/data2/wmq/editing-data/vqa/vqa_eval.json

# Evaluation
all_metrics_name: 'all_metrics_ada_layer7_updown.jsonl'
json_dir: /mnt/data2/wmq/EasyEdit/results/jsonl
continuous_sample: 1

real_world_eval: true
api_key: "/mnt/data2/wmq/api_key.json"