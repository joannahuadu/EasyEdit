alg_name: "LoRA"
# name: liuhaotian/llava-v1.5-7b
name: /root/autodl-tmp/model/4481d270cc22fd5c4d1bb5df129622006ccd9234
model_name: "llava"
model_class: LlamaForCausalLM
tokenizer_class: LlamaTokenizer
# tokenizer_name: liuhaotian/llava-v1.5-7b
tokenizer_name: /root/autodl-tmp/model/4481d270cc22fd5c4d1bb5df129622006ccd9234
cache_dir: /root/.cache/huggingface/hub/
device: 0,1

lora_type: "adalora"
layers: [7]
num_steps: 70
batch_size: 1
max_length: 30
lr: 5e-3
weight_decay: 0
kl_factor: 0
rank: 8
lora_alpha: 32
lora_dropout: 0.1
norm_constraint: false
target_modules: ["up_proj", "down_proj"] #["q_proj", "v_proj"]  #["up_proj", "down_proj"] #["q_proj", "v_proj"]
model_parallel: false

# image
coco_image: /root/autodl-tmp/model_edit
rephrase_image: /root/autodl-tmp/model_edit
result_dir: /root/autodl-tmp/model_edit/results
train_annotation_path: /root/autodl-tmp/model_edit/editing-data/vqa/vqa_train.json
eval_annotation_path: /root/autodl-tmp/model_edit/editing-data/vqa/vqa_eval.json

# Evaluation
all_metrics_name: 'all_metrics_ada_layer7_updown_corda.jsonl'
json_dir: /root/code/EasyEdit/results/jsonl
continuous_sample: 1

real_world_eval: true
api_key: "/root/autodl-tmp/api_key.json"