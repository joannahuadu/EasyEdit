alg_name: "LoRA"
name: /public/home/wang_mq22/.cache/huggingface/hub/4481d270cc22fd5c4d1bb5df129622006ccd9234
model_name: "llava"
model_class: LlamaForCausalLM
tokenizer_class: LlamaTokenizer
tokenizer_name: /public/home/wang_mq22/.cache/huggingface/hub/4481d270cc22fd5c4d1bb5df129622006ccd9234
cache_dir: /public/home/wang_mq22/.cache/huggingface/hub/

device: 0

lora_type: "lora"
layers: [7]
num_steps: 70
batch_size: 1
max_length: 30
lr: 5e-4
weight_decay: 0
kl_factor: 0
rank: 8
lora_alpha: 32
lora_dropout: 0
norm_constraint: false
target_modules: ["up_proj", "down_proj"] #["q_proj", "v_proj"]  #["up_proj", "down_proj"] #["q_proj", "v_proj"]
exclude_modules: []
model_parallel: false 

# image
coco_image: /public/home/wang_mq22/edit_data
mmke_image: /public/home/wang_mq22/edit_data/MMKE/data_image
rephrase_image: /public/home/wang_mq22/edit_data
result_dir: /public/home/wang_mq22/edit_data/results
train_annotation_path: /public/home/wang_mq22/edit_data/editing-data/vqa/vqa_train.json
caption_train_annotation_path: /public/home/wang_mq22/edit_data/editing-data/caption/caption_train_edit.json
eval_annotation_path: /public/home/wang_mq22/edit_data/editing-data/vqa/vqa_eval.json
mmke_train_annotation_path: /public/home/wang_mq22/edit_data/MMKE/data_json/entity_train.json

# Evaluation
all_metrics_name: 'lora_layer7_updown.jsonl'
json_dir: /public/home/wang_mq22/EasyEdit/results/jsonl
continuous_sample: 1

real_world_eval: true
api_key: "/public/home/wang_mq22/edit_data/api_key.json"